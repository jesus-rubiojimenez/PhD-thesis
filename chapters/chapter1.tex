\chapter{Introduction}
\label{chap:intro}

The primary empirical idea that underlies science in general and physics in particular is the existence of a part of the world that is independent of the mental activity of human beings. When this external world affects us and is affected by our actions in any way that we can perceive, we say that an interaction has taken place, and that this happens is precisely what gives us the opportunity of enquiring about the fundamental nature of reality.

To understand how this is possible, first we note that, to some extent, we have the ability to divide our perception of the external world as if it were a collection of different entities that can be categorised. In addition, by noticing that human beings are just another part of the world, we can extend the idea of interaction to apply between any two or more of these entities. Therefore, we can always choose a small set of entities that is delimited in space and time, which we call a system, and we can generate a chain of local interactions between them in order to study how the system behaves in a controlled way. This is what an experiment does.

For this process to be successful, it is crucial that we can recognise how the system changes under the influence of our actions, and the appearance of these changes is what we call events. A careful exposition of the central role that events play in physics can be found, for instance, in the treatments of Englert \cite{englert2013} and Haag \cite{haag1996, haag2016}. Crucially, the chain of actions that ends in the creation of an event does not need to involve a human observer directly manipulating the system of interest at every stage. For example, we can consider an interaction between the system and some artificial object that only interacts with us at a later stage. As a consequence, while it is true that which events we observe and how we do it depend on the actions that we choose to implement in the laboratory, it may be considered that the physical nature of a given event does not rely on the observer, and in that sense we can say that events give rise to objective facts. An example that illustrates well this point is that of atomic spectra, which are independent of who is performing the experiments that create the events that determine them.

The study of the events that we can observe allows us to speak about natural phenomena in terms of relationships between physical properties, and this provides us with a basis to construct the theories that help us to explain the world around us. Although as a first approximation we can describe such properties using verbal statements, a greater precision (and often clarity) is achieved by capturing their essence with mathematical objects and relationships. Of particular importance are quantitative representations, which associate numbers with properties. One of the two fundamental procedures to accomplish this task is to measure. 

Measurements can be regarded as collections of interactions between the system and some device, such that the properties that emerge from the events created in this way can be associated with magnitudes by comparison with respect to standard references or units \cite{vim2012}. Depending on the experiment, properties are sometimes quantified using integers or rational numbers, and in general they can be characterised by a set of real numbers, each of them lying within some interval whose width is related to the experimental conditions of the measurement. The use of real numbers is particularly useful in this context, since the fact that they form a complete ordered field\footnote{Here \emph{field} refers to the mathematical concept (see, e.g., \cite{spivak1980}), and not to the physical object that will appear in chapter \ref{chap:conceptual}.} \cite{spivak1980} provides us with a powerful tool for the practical necessity of comparing any two given magnitudes that may vary in a smooth way. The part of science that studies the design and implementation of measurements is metrology \cite{vim2012}.

Unfortunately, not all physical properties can be associated with a measurement procedure in such direct fashion. In fact, most of them require more complicated schemes. This is the case, for example, when we need to assign a value to a parameter that represents the difference of optical phases, which is one of the central scenarios that we will study in this thesis. In that case, our measurement is based on the detection of light \cite{helstrom1976, rafal2015, dowling2014}, and a difference of phases can only be quantified by means of a theoretical relationship that connects such parameter with the outcomes of the measurement. We then say that the parameter has to be estimated. 

The estimation of physical parameters has a wide range of applications. These include the important case of generating an educated guess for the value of those properties without a direct measurement scheme, or for which a measurement scheme is difficult to implement, and also the task of connecting parameters of a theoretical nature with experimental procedures, which is crucial to assigning values to the constants that characterise a theory. We see then that an estimation procedure is the second fundamental way of assigning numerical values to physical properties, and the formal framework that studies how to construct useful estimation schemes is estimation theory \cite{helstrom1976, jaynes2003, kay1993}. Note, however, that while estimation theory opens the door to quantify a larger set of properties, it still relies on the direct measurement of more primitive quantities that are to be related to the property of interest. 

When we succeed in assigning numerical values to some property, we say that we have extracted information from the natural world. Sometimes this assignment is unique, but in most cases we can only find a set of possible values that are compatible with the property that we are studying, and a collection of weights indicating how likely each value is on the basis of what we know about the situation at hand. When this happens, the information that we gain is partial in the sense that it does not lead to a unique answer. This ambiguity is captured by the notion of uncertainty, and the partial information that is available is modelled via the concept of probability \cite{jaynes2003}. Importantly, note that partial information as defined here does not necessarily imply a notion of incompleteness, since partial information may be all that a given system is able to offer for the property that we are trying to associate with it. 

The previous discussion highlights the crucial importance of our ability to extract information in the generation, development and testing of our scientific knowledge, and this motivates searching for new ways of enhancing both our measurement and estimation techniques. This is, in a general sense, our key motivation for addressing the research question that we develop in the following paragraphs.  

One of the most powerful known ways of improving how we measure and make estimates is to exploit the fundamental role that quantum mechanics - which is one of our fundamental theories about the universe - plays in the technological revolution known as \emph{the second quantum revolution} \cite{dowling2003}, whose aim is to exploit the physical principles of quantum theory for the development of new technologies in the areas of sensing, computation and communication \cite{degen2017, browne2017, barnett2017, acin2018}. These include important applications such as gravitational wave detection \cite{aasi2013, pitkin2011}; medical and biological imaging \cite{taylor2013, taylor2015, taylor2016}; measurements for other fragile systems such as atoms, molecules or spin ensembles \cite{eckert2007, pototschnig2011, carlton2010, wolfgramm2013, PaulProctor2016}; magnetic sensing \cite{baumgart2016}; quantum radar \cite{shabir2015, kebei2013, lanzagorta2012} and lidar \cite{lanzagorta2012, wang2016, zhuang2017}; navigation \cite{Dowling1998}; and quantum networks for distributed sensing \cite{proctor2017networked, proctor2017networkedshort, ge2018, eldredge2018, altenburg2018, qian2019} and for satellite-to-ground cryptography \cite{liao2017}, among others. In general, quantum technologies can be classified depending on whether information is extracted, processed, transferred or stored \cite{samuel2018}. 

Despite this broad scope, the common denominator underlying all quantum technologies is precisely the possibility of performing high-precision measurements, since as Dowling and Milburn argued in \cite{dowling2003}, that is a crucial requirement for the success of any technology. This gives rise to the application of quantum mechanics to the enhancement of our measurement techniques, and this process initiates a very useful feedback loop whose logic was particularly well captured by the reasoning advanced by Dunningham in \cite{dunningham2006}: measurements provide the basic information to create theories, and those theories allow us to find better measurement techniques that, in turn, might give rise to a whole new theoretical framework. The result of this is a theory to design and implement measurements by exploiting the quantum properties of light and matter. In other words, we have a \emph{quantum metrology}.   

From a formal perspective, quantum metrology can be seen as a collection of techniques that rely on quantum mechanics in order to extract information about unknown physical quantities from the outcomes of experiments \cite{giovanetti2006review, dunningham2006, paris2009, rafal2015}. Expressed in this way, its final aim is to find the strategy that can extract information with the greatest possible precision for a given amount of physical resources, and, as a consequence, it sets an optimisation problem. To solve it, first we need to define some mathematical quantity that acts as a figure of merit and informs us about the uncertainty of the estimation process, and then we can minimise such quantity with respect to the features that we can typically control, which include the details of the preparation of the experiment, the measurement scheme itself and the statistical functions employed in the analysis of the experimental data to generate an estimate. 

In practice the quality of the information extracted in this way is restricted by factors such as the number of probes, measurements or repetitions of the experiment, or by the energy that the experimental arrangement can employ. The latter constraint is particularly relevant for cases where we are interested in studying fragile systems \cite{eckert2007, pototschnig2011, carlton2010, taylor2013, taylor2015, taylor2016, PaulProctor2016}. On the other hand, the number of times that we can interact with the system under study by performing several measurements is always finite and potentially small. This is a possibility that could arise, for instance, in tracking scenarios where a scheme for remote sensing can only have access to a few observations before the object of interest is out of reach \cite{shabir2015, kebei2013, lanzagorta2012,wang2016,zhuang2017}.

It is also important to appreciate that the measurement data is not the only source of information that we can use to make estimates. To formulate an estimation problem we normally need to further specify certain details such as the instructions that we must follow in order to implement some metrology scheme in the laboratory, or any other piece of information that we may have about the unknown parameters whose values we wish to learn, the origin of such information being different from the measurement data. An example of the latter is the range of possible values that such parameters could take. This kind of information is said to be known a priori, in the sense that it precedes the initialisation of the experiment.

Given this state of affairs, it is crucial to observe that quantum metrology protocols are typically designed around the assumption that we have an abundance of measurement data (see, e.g., \cite{braunstein_gaussian1992, rafal2015, braun2018, tsang2016, liu2016, lumino2017}), and this clearly excludes real-world scenarios with very limited data such as those mentioned above. Moreover, it is frequent to find studies where it is assumed that we are working either in the high prior information regime \cite{paris2009, rafal2015}, or in the presence of complete ignorance \cite{ariano1998, chiara2003, chiribella2005, holevo2011, rafal2015}, which is the other extreme, while we may expect a realistic amount of prior knowledge to normally be moderate. In addition, these assumptions are also unsatisfactory from a theoretical point of view. Indeed, we will see that the mathematical consequence of assuming a large number of data is that the framework derived from such premise is generally valid only in an asymptotic sense, and that imposing a very large amount of prior knowledge effectively restricts the validity of our estimates to a local region of the parameter space. On the contrary, an approach of a more fundamental nature should ideally not rely on an asymptotic approximation to be relevant and useful, and it should allow for the possibility of accessing any regime of prior knowledge. 

Therefore, there is an unmet need of developing methods to study and design metrology protocols that operate in this largely unexplored and more realistic regime. This directly leads us to our thesis, which we now enunciate as follows:
\begin{framed}
\justify{~\\[-20pt]The number of times that we can access a physical system to extract information via quantum metrology and estimation theory is \emph{always} finite, and possibly small, and a realistic amount of prior information will typically be moderate. As a consequence, theoretical consistency demands a quantum metrology methodology that can depart from both asymptotic approximations and restricted parameter locations, while practical convenience requires that such methodology is also sufficiently flexible and easy to use in applications where the amount of data is limited. We submit that this methodology can and should be built on the Bayesian framework of probability theory, and that its construction can be carried out and adapted for both single and multi-parameter schemes, including important models such as the Mach-Zehnder interferometer and quantum sensing networks. Finally, we advance that this methodology generates a wealth of new results characterising an interesting interplay between different amounts of data, the prior information and quantum correlations. In other words, we propose, construct, explore and exploit a \emph{non-asymptotic quantum metrology}.}
\end{framed}

The first step to accomplish our goals will be the introduction of the fundamental concepts that we need to develop our ideas, a task that will be carried out in chapter \ref{chap:conceptual}. We will start by arguing that a version of probability theory where the focus lies on the information content of our probability models is the most suitable choice for studying the regime of limited data, and we will review the basic elements of this approach. Once we have the means to model information in terms of probabilities, we will proceed to study how such information can be encoded in quantum systems, and we will revisit the formalism of quantum mechanics to verify that using the version of the Bayesian framework that we consider in this thesis is compatible with the usual notions in quantum theory. We then complete our conceptual framework presenting the characterisation of a generic Mach-Zehnder interferometer and the quantum sensing network model introduced by Proctor \emph{et al.} \cite{proctor2017networked}, where the latter is the type of multi-parameter scheme that we will examine. 

At this point we will have all the ingredients to formulate the problem of this thesis in a formal way, which will happen in chapter \ref{chap:methodology}. We will focus our attention on experiments that are repeated a certain number of times, and upon introducing a notion of resources that is relevant for our purposes, we will define the regime of limited data in terms of a low number of trials. Then we will carry out a detailed analysis of different measures of uncertainty that we could consider as the figure of merit to be optimised, and a measure of uncertainty that is appropriate for designing inference schemes from theoretical considerations will be selected. 

Equipped with this uncertainty, we will review the fundamental equations for the optimal quantum strategy in a Bayesian context \cite{helstrom1976, helstrom1974, holevo1973b, holevo1973}, and also a set of bounds (including the widely used Cram\'{e}r-Rao bound \cite{paris2009, rafal2015}) that are often employed and that can be useful due to the general difficulties to solve the previous equations, and we will highlight the assumptions that go into the construction of these tools. Furthermore, we will revisit some known results in the single-shot regime, and we will demonstrate that a new way of understanding the latter is possible by explicitly separating the quantum and classical steps during the process of optimising the single-shot uncertainty. This will be one of our first novel insights. 

The analysis of the advantages and potential drawbacks of different tools will prepare the ground for the construction of our non-asymptotic methodology, which will emerge as a useful method for quantum metrology that is more general than simply using bounds, and while it is not as general as solving the fundamental equations for the optimal strategy, our methods will be associated with calculations that are more tractable than those in the latter approach. 

Two key approaches will constitute the basis of this methodology. We will always select the estimator that is optimal for any number of repetitions, such that this part of the problem is always exact in all our calculations, and we propose two different methods to select the quantum strategy, i.e., how the system is to be prepared and which measurement scheme should be selected. On the one hand, we propose to employ the known asymptotic theory as a guide, and to choose a quantum strategy that is guaranteed to be optimal as the number of repetitions grows, even when the analysis of the scheme is done with an uncertainty that also works in the non-asymptotic regime. That is, if we were not sure a priori about how many times the experiment is to be repeated, a weak condition that we could impose on the optimisation would be that the performance of the scheme should not break in the long run. While this does not guarantee that the solutions arising from this method will be optimal for a low number of trials, we will see that this is a useful approximation that will allow us to extract some information about the non-asymptotic regime of our metrology schemes.

As for the second method, we will go a step further and construct a fully Bayesian approach based on selecting the quantum strategy that is optimal for a single shot, so that this scheme is then repeated as many times as the application at hand demands or allows for. In this way only the resources that are needed will be optimised, and the necessity of relying on tools that assume a large amount of data will be completely eliminated from our calculations. Hence, chapter \ref{chap:methodology} will lay a bridge between the current state of the art and the novel ideas that our work introduces. 

The next four chapters will be dedicated to developing the theory associated with our non-asymptotic methodology by dividing this process in four different steps, one per chapter, and we will demonstrate these ideas explicitly with specific metrology schemes. The first step is to construct the hybrid method (exact estimation plus asymptotically optimal quantum strategy) for single-parameter schemes, a task that will be carried out in chapter \ref{chap:nonasymptotic}. Then we will demonstrate its usefulness in the context of a Mach-Zehnder interferometer that has been prepared using current techniques in optical interferometry, and we will use our method to address two questions: when does the Cram\'{e}r-Rao bound stop being valid, since in general it is only meaningful in an asymptotic sense, and how the validity of predictions of such tool change when the experiment is operating in the non-asymptotic regime. Our results will verify that the number of repetitions and the minimum amount of prior information needed to recover the asymptotic behaviour crucially depend on the state of the system. In addition, we will propose a simple analytical relation to identify and prevent the appearance of states for which the number of trials required to match the asymptotic uncertainty grows unbounded, while, at the same time, almost no information is gained for a low number of repetitions.

Our study of the Mach-Zehnder interferometer will continue in chapter \ref{chap:limited}, but this time we will implement our second method, that is, the optimisation of the uncertainty in a shot-by-shot fashion. We will see that this technique generates bounds on the estimation error that can be tight both for a single shot (by construction) and for a large number of them, since the predictions of the Cram\'{e}r-Rao bound are sometimes recovered as a limiting case within our approach. This partially fundamental character will further allow us to provide the first rigorous characterisation of the interplay between the amount of data, the prior information and the photon correlations associated with the interferometer, fulfilling in this way one of the main claims of our thesis for single-parameter protocols. Remarkably, we have found evidence of the potential existence of a trade-off between the asymptotic and non-asymptotic performances that is associated with the photon correlations within each optical mode. More concretely, while a large amount of the latter is beneficial asymptotically, sometimes it appears to be detrimental for a low amount of data. Moreover, our bounds provide us with a new benchmark to study whether certain practical measurements are actually optimal in the regime of limited data, and we have shown that the bounds that emerge from our technique are superior to other alternatives in the literature such as the quantum Ziv-Zakai and Weiss-Weinstein bounds \cite{tsang2012, tsang2016} whenever we restrict our attention to identical and independent experiments. As a final demonstration of the power of our single-parameter approach we have combined our methods with a genetic algorithm for state engineering that has been developed by our colleagues at the University of Nottingham, and we have shown that our Bayesian methodology can predict schemes that not only supersede standard benchmarks, but that have the potential to be experimentally feasible. 

The transition from single to multi-parameter estimation problems is made in chapter \ref{chap:networks}. Here we return to the hybrid method where the quantum strategy is asymptotically selected and we extend it to cover cases with several parameters. Once this step has been achieved, we proceed to apply it to a collection of sensors that are spatially distributed, which can be modelled with the framework for quantum sensing networks developed by Proctor \emph{et al.} \cite{proctor2017networked, proctor2017networkedshort}. The presence of several parameters opens the door to a vast set of new possibilities to enhance our estimation protocols, and for that reason it is useful to introduce some definitions that help us to identify in a transparent way what our final goal is. To that end we define, on the one hand, the notion of natural or primary properties of the network, and, on the other hand, the concept of derived or secondary properties, where the former refers to the original parameters of the system and the latter to functions of them. Our task in this chapter is then to determine which role the correlations between sensors play in the estimation of global properties, where these are understood as linear functions that depend non-trivially on several parameters that were originally encoded in a local way. Assuming a network where each node is a qubit, we first solve this problem asymptotically to extract the solutions that will serve us as a guide at a latter point. In particular, we will uncover the link between the geometry of the vectors associated with the components of the linear functions and the amount of inter-sensor correlations that are needed for achieving the asymptotically optimal error, and we will show that how much entanglement is required for a given geometry crucially varies with the number of repetitions of the experiment, which is a result fully compatible with our findings in the non-asymptotic study of the Mach-Zehnder interferometer. 

The final step of our methodology, which is implemented in chapter \ref{chap:multibayes}, will focus on generalising the shot-by-shot method to the multi-parameter regime. To achieve this goal we will first derive a new multi-parameter single-shot quantum bound, and we will show under which circumstances it can be saturated. This is perhaps one of the most important results that we report in this thesis. We will calculate this bound both for the qubit network studied in chapter \ref{chap:networks} and for a discrete model for phase imaging, and we will show that entanglement is not needed for the estimation of the original parameters of the network when the experiment operates in the regime of moderate prior knowledge and limited data. The crucial importance of this finding stems from the fact that an analogous result had only been established in a clear way in terms of the asymptotic theory \cite{knott2016local, proctor2017networked, proctor2017networkedshort}. 

In chapter \ref{chap:future} we will identify some of the limitations of our current approach and will discuss some ideas to overcome them, as well as potential ideas for the future of non-asymptotic quantum metrology, while chapter \ref{chap:conclusions} will be dedicated to the analysis of the unified perspective that will emerge from the findings and conclusions presented in previous chapters. 

Finally, we would like to draw attention to the fact that in appendices \ref{app:numsingle} and \ref{app:multinum} we provide a comprehensive numerical toolbox for optical interferometry and two-parameter estimation problems that is based on MATLAB and Mathematica algorithms. Hence, the interested readers will have the opportunity of either reproducing our results or adapting our algorithms to their specific problems. The relative simplicity and efficiency of these algorithms might help to overcome the extended perception that Bayesian techniques, while often conceptually clearer, are somehow less accessible due to the numerical character of the associated calculations. For the details of some of our analytical calculations and extended discussions about our methods, see appendix \ref{app:supplemental}.